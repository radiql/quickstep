# Motivation for RADiQL QuickStep

In traditional old-fashioned Agile development, User Stories are implemented one by one with a collection of classes, functions, objects, schemas, APIs and a plethora of frameworks, libraries and technology decisions. The development team frames its thoughts in terms of the implementation technologies, available APIs and architecture already selected first and foremost rather than the pure functionality described by the User Stories and the flow of data that joins these User Stories together.

## The cost of User Story reimplementation

An application is treated as a set of custom User Stories the like of which have never been seen before and that need to be developed from scratch within the context of some runtime environment or process (such as a microservice). A characteristic of this approach is that there is virtually no re-use of User Stories from application to application, nor from microservice to microservice. 

## <a name="risk-management"/>Suboptimal risk management with no prototyping

An important principle of Agile methodologies is development of an application through many small incremental steps and short development cycles that are often referred to as "sprints", as they're referred to when adopting Scrum. Sprints typically last for between one and three weeks though shorter sprints and longer sprints are possible. The golden rule is that there must be working software at the end of each sprint. During each sprint a number of User Stories will be identified as within scope of the sprint then converted into tasks for coding and testing.

The problem here is that Agile teams focus on "building stuff" with a production implementation of the User Stories. Little or no time is given to prototyping the functionality since this is impractical given the Agile team skill set and the mix of technologies typically used in traditional Agile development. These technologies are typicially a textual programming langage (such as C, C++, JavaScript, Scala, Java, C#, Ruby, Clojure or whatever), a runtime environment (Node, JVM, .NET, etcetera.) and a database (Oracle, Neo4J, Cassandra, MySQL, MongoDB, etcetera). Using Test Driven Development (TDD) or a similar approach, unit tests are written by hand, functionality implemented and the functionality tested in a QA environment using an automated test framework such as Selenium or similar. 

Because development cycles are short it's thought that there is no point performing prototyping. However, keeping development cycles short reduces the risk but doesn't minimize it nor eliminate it. The principle is that the financial risk is restricted to one to three weeks worth of Agile team costs. If the requirements are proved to be wrong or insufficiently demanding or have been incorrectly implemented at the end of the sprint, then User Stories are created for defects or rework.

Prototyping is about reducing exposure to risk to days, hours or minutes rather than weeks or months. The ideal solution is where prototyping produces a production quality solution with little or no need to code by hand. Manual coding, unit testing and QA testing aren't eliminated entirely, but when new functionaltiy is required and needs to be implemented this way it should be made available within a repository and reusable for any future iteration of rapid prototyping that involves similar functionality. Development, testing and documention costs are thus largely eliminated for future reuse of the User Story implementation. This time can instead be spent developing new functionality only where this is needed and/or reducing time to market and project costs.

## Too much code and excessive test impact

Test driven development is an important tool in the Agile developer's armoury. When writing code by hand, writing the test for the code first and ensuring that from the outset the test fails is a pre-requisite. When the test passes, the code is complete. Done properly, TDD should achieve almost 100% code coverage.

The real issue here is not with TDD at all. It's the fact that human beings make mistakes when writing new code. TDD keeps them honest and overall more productive than if no unit tests are written. But the underlying problem (and often the almost invisible elephant in the room) is that developers simply write too much code and new code that's written by humans always has to be tested.

A further problem is that TDD is often applied in the wrong place. The unit is assumed to be a class or a function rather than an encapsulating interface, boundary object, package or module. The problem of testing at the wrong level is that should code need to be refactored, the unit test often needs to be rewritten. This destroys the theoretical value of the test to ensure functional equivalence of the old and new implementations of the functional code. In other words, if you have to write the unit test again then you really haven't done any valid regression testing because the test implementation has itself changed and the developer is just as much likely to makes oversights changing the test as would be the case if it were the code under test. The test shouldn't have changed at all. So it's important to put testing in the right place and reuse it when the implementation is re-factored.

What would be better for most of the time is if human beings were not involved in writing new code. Currently human beings are difficult to eliminate from the loop. But the key point to make here is that by driving up component reuse based upon entire User Stories (rather than low level functions and classes) we can achieve this to a much higher degree and minimise or eliminiate the impedance mismatch between the User Story requirement and the correctness of the functional implementation of this in code. 

Furthermore, by reducing our testing to inputs and outputs of the User Story implementation as pure data, we greatly simplify the testing process and make the test portable across languages and environments. This means that even if the code is reimplemented in a different language, the test implementation can stay language neutral. This means that one test implementation can be used to test implementations of code in many languages (JavaScript, Java, Scala, C#, etcetera). 

Its worthwhile noting that in high technology manafacturing (from which the software development industry has only recently adopted the principle of bulk automated testing) it is recognised that test is a cost and that cost should be eliminated wherever possible by process control. In other words, by improving the process, the necessity of the test disappears. Stated differently, if your process is good enough then the test becomes redundant. Tests are not a measure of quality. They are a measure of lack of quality. Tests are necessary in practice but more tests are not a good thing. This is perhaps counterintuitive but it is a truism because the only useful test is one that fails. If a test fails then the process that produced the product is flawed.

What manufacturing learned many years ago, starting in Japan with Total Quality Management at Toyota, is that it is necessary to find a way of controlling the process and to improve upon the process itself to guarantee the quality of the product. Structured programming and widescale human involvement in the process, means that the process will continue to have the same flaws for ever, which is what makes writing tests necessary. Flow Based Programming and the practice of reducing the impedance mismatch between User Stories or Use Cases and the unit of implementation, means that automated assembly and composition of prebuilt low-level use-cases using machine learning becomes a realistic solution. 

Programming languages are only for the benefit of human beings who are trying to explain (and keep a record of) instructions to a machine. Business analysts or product owners translation requirements into User Stories. Programmers translate User Stories into an implementation language and this implementation language is itself then translated into machine instructions. Transcription errors (mistakes) made during each translation step are the source of defects. By reducing the number of translation steps and by reducing the variability introduced by the natural lack of consistency associated with human beings, the process is by definition improved. Whole categories of errors are removed.

## Data Monitoring and Separation of Concerns

Introducing data monitoring often requires dedicated tooling that's relatively non-portable. For this reason, many data monitoring solutions monitor HTTP requests and responses and the interface of a microservice or API gateway. The problem is that it's difficult to inject the monitoring solution into compiled code.

The inputs and outputs of a User Story are, however, possible to define with great precision in order to define portable test cases. And if User Story implementations are self-contained and plugable, it's possible to intercept streams of data between User Story implementations and tap the data stream for monitoring purposes. It then becomes easy to integrate monitoring from various data streams since all are carrying Information Packets (IPs) that follow a particular structure and obey the same conventions. The data in the data stream is pure, in other words. It isn't encumbered by the implementation details and calling conventions of any particular language. 

## Slow time to market

With re-implementation of common User Stories comes slower time to market and less complete Minimum Viable Products (MVPs). User Stories are typically implemented and re-implemented from scratch, burning time during both development and testing. Existing User Story implementations cannot easily be refactored. Where a User Story has been implemented in another language or technology, this often cannot be easily integrated into a new microservice of application due to imcompatibilities in the technologies.

## Problems of User Story portability

Switching from a monolithic application to a suite of microservices (or back again) becomes very complex because the implementations of User Stories are now no longer portable. Invocation of remotely hosted functionality (via HTTP web service calls protected by invasive circuit breaker implementation code such as Hystrix) looks very different to a local method call on another class running in the same process.

This lack of portability is due to the very hard dependencies that have been built on languages, frameworks and database technologies as well as method and function signatures. It's also due to the interweaving of User Story implementations in the code. User Story implementations rapidly become interdependent and are woven together over time. In other words there is very high coupling between User Stories and, as a result, the code becomes brittle when trying to extract or refactor User Story functionality at a later date. 

## Problems of poor documentation

Agile is typically characterised by focussing on a low ceremony approach that minimises documentation during developement and where the code itself becomes the documentation. The theory goes that expert product-focussed teams will retain intimate knowledge of the code and retain an agile edge when being required to make changes. When microservice approaches are adopted, the theory goes that the smaller finer-grained microservices will be easier to understand (as well as more flexible to redeploy and upgrade). In practice though, things rarely work out quite like this and this is expecially true when measure over years.

The result is that functionality, which was originally driven by documented user stories, is now all mixed up and baked into an application or microservice. The relationship between microservices and the multiple applications that may use them is not well understood either. And virtually none of these interdependencies will be documented. And even if they are at the outset, it's unlikely that this will be true in the long term.

With regard to self-documented code, the very directory structure of an application or microservice will be dominated by directories or folders with names like "controller", "view", "model", "helpers", "services", "persistance" and so on and so forth. A glance through the directory structure, package names or module names rarely, if ever, tells the story of what the application or microservice is actually doing (and how). 

Each User Story is broken up and distributed into artificial technical constructs dictated by convention (for a particular, language, technology or framework) or by design pattern. The User Story is no longer documented effectively in the code because the story has been sliced and diced and distributed across multiple files and possibly translated into several programming languages. The User Story itself and the correctness of its implementation is therefore very difficult to follow by reading the code. The code may document itself by telling us what its doing and how. But it rarely, if ever, tells us why its doing this, why its organised in that way and what interdepencies exist with other functionality in the same process and with other functionality in remote processes.

In a world where distributed systems are becoming the norm, relying on the code alone to document the implementation of User Stories (that may span multiple processes) is completely inadequate. 

## Knowledge loss and high cost of maintenance

It's the lack of User Story portability that makes long term maintenance of applications and microservices expensive. Developers and technology managers eventually fear making changes to applications and microservices due to the high risk of introducing new defects through lack of understanding of how User Story implementations have been interwoven and due to the interdependencies that are now no-longer understood. JIRA and Git provide a degree of traceability, but the complexity of re-tracing the original development team's steps using using JIRA and Git makes separating User Story implementations effectively impossible. Furthermore the test impact is typically quite considerable.

Staff turnover due to the modern reality of flexible resourcing and the wide variety of opportunities in the areas where development centres tend to be located (such as London, New York or the San Francisco area), along with the low-ceremony, low documentation of most Agile shops means that risk of introducing changes into an application or microservice 3 or 4 years down the line becomes very a very daunting prospect.

## <a name="inflexible-granularity"/>Wrong and inflexible granularity of deployment

In addition to User Story implementations being split up into a number of components and services that may themselves be combined with other User Stories and therefore providing limited business agility, components and services are often deployed grouped together into a runtime process from the outset of development. Typically an Agile team will be responsible for one or more microservices in a microservice architecture and will therefore implement a User Story within the context of a particular microservice. As we've already discussed, this knock on effects of this for other consumers of the microservice is that remote and local code looks very different and therefore portability of the functionality (both from the client and the server perspectives) is badly compromised.

However, this is a wider problem. Because the User Story implementions (that are distributed across multiple files in different application layers and possibly even different processes entirely in a distributed environment) collectively describe the business rules. This means that business rules cannot be easily and conveniently made available to third parties or third party products. The implications for government, for example, are that government business rules (such as tax calculations or complex data validation) need to be re-implemented by third party software vendors and organisations. Re-implementation implies a significant test impact and the necessity of some sort of government-provided test environment with which the correctness of the third party implementation can be tested. This incurs additonal cost for both parties both in terms of money and time.

This is a wasteful use of resources and represents higher costs for the taxpayer and the consumer of the third party software. Furthermore, it restricts how much the state can offload the burden of software development onto the private sector. A knock-on effect is that it restricts competition and innovation in the private sector for this market.



